{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv(\"confirmed_t.csv\")\n",
    "deaths = pd.read_csv(\"deaths_t.csv\")\n",
    "recovered = pd.read_csv(\"recovered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCols = list(recovered.columns[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_confirmed = confirmed[dataCols].values\n",
    "X_deaths = deaths[dataCols].values\n",
    "X_recovered = recovered[dataCols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(values):\n",
    "    mean = values.mean(axis=1).reshape(-1,1)\n",
    "    std = values.std(axis=1).reshape(-1,1)\n",
    "    return (values - mean) / (std+1e-9), mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_confirmed, mean_confirmed, std_confirmed = normalize(X_confirmed)\n",
    "X_deaths, mean_deaths, std_deaths= normalize(X_deaths)\n",
    "X_recovered, mean_recovered, std_recovered = normalize(X_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size, single_step=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset[0]) - target_size\n",
    "        \n",
    "    for d in dataset:\n",
    "        for i in range(start_index, end_index):\n",
    "            indices = range(i-history_size, i)\n",
    "            # Reshape data from (history_size,) to (history_size, 1)\n",
    "            data.append(np.reshape(d[indices], (history_size, 1)))\n",
    "            if single_step:\n",
    "                labels.append(d[i+target_size])\n",
    "            else:\n",
    "                labels.append(d[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 20\n",
    "PRED_DAYS=3\n",
    "X_confirmed_train, y_confirmed_train = univariate_data(X_confirmed, 0, None, HISTORY_SIZE, PRED_DAYS, False)\n",
    "X_deaths_train, y_deaths_train = univariate_data(X_deaths, 0, None, HISTORY_SIZE, PRED_DAYS, False)\n",
    "X_recovered_train, y_recovered_train = univariate_data(X_recovered, 0, None, HISTORY_SIZE, PRED_DAYS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28336, 20, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_confirmed_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(32, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3))\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_confirmed = build_model(X_confirmed_train.shape[-2:])\n",
    "model_deaths = build_model(X_deaths_train.shape[-2:])\n",
    "model_recovered = build_model(X_recovered_train.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "def make_dataset(X, y):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "    return dataset\n",
    "    \n",
    "train_data_confirmed = make_dataset(X_confirmed_train, y_confirmed_train)\n",
    "train_data_deaths = make_dataset(X_deaths_train, y_deaths_train)\n",
    "train_data_recovered = make_dataset(X_recovered_train, y_recovered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 0.2408\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.1167\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0876\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0722\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0644\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0595\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0561\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0538\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 0.0513\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.0498\n",
      "Train for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.2056\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.1001\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0826\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0724\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0655\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0614\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0583\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0549\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0526\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0511\n",
      "Train for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.2157\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.1045\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0891\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.0786\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0727\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.0665\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0638\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.0617\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.0593\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.0577\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "EVALUATION_INTERVAL = 200\n",
    "def train_and_predict(model, train_dataset, test_dataset, mean, std):\n",
    "    model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL)\n",
    "    pred = model.predict(test_dataset)\n",
    "    pred = pred * std + mean\n",
    "    return pred\n",
    "    \n",
    "test_data_confirmed = X_confirmed[:,-HISTORY_SIZE:].reshape(-1,HISTORY_SIZE,1)\n",
    "test_data_deaths = X_deaths[:,-HISTORY_SIZE:].reshape(-1,HISTORY_SIZE,1)\n",
    "test_data_recovered = X_recovered[:,-HISTORY_SIZE:].reshape(-1,HISTORY_SIZE,1)\n",
    "    \n",
    "pred_confirmed = train_and_predict(model_confirmed, train_data_confirmed, test_data_confirmed, mean_confirmed, std_confirmed)\n",
    "pred_deaths = train_and_predict(model_deaths, train_data_deaths, test_data_deaths, mean_deaths, std_deaths )\n",
    "pred_recovered = train_and_predict(model_recovered, train_data_recovered, test_data_recovered, mean_recovered, std_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def append_pred(dataframe, pred):\n",
    "    enddate = datetime.datetime.strptime(dataCols[-1], '%m/%d/%y')\n",
    "    prevdate = enddate\n",
    "    for i in range(1, PRED_DAYS+1):\n",
    "        nextdate = enddate + datetime.timedelta(days=i)\n",
    "        temp = np.concatenate([dataframe[prevdate.strftime('%m/%d/%y')].values.reshape(-1,1), pred[:,i-1].reshape(-1,1)], axis=1)\n",
    "        dataframe[nextdate.strftime('%m/%d/%y')] = temp.max(axis=1).astype(np.int32)\n",
    "        prevdate = nextdate\n",
    "        \n",
    "append_pred(confirmed, pred_confirmed)\n",
    "append_pred(deaths, pred_deaths)\n",
    "append_pred(recovered, pred_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed.to_csv('confirmed_with_pred.csv', index=False)\n",
    "deaths.to_csv('deaths_with_pred.csv', index=False)\n",
    "recovered.to_csv('recovered_with_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
